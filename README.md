I created this project to learn neural nets at a lower level, as I couldn't find a satisfying summary of how backprop worked; most articles had ungodly complicated nested sigmas with no big picture. I've done this in stages.
1. back_prop.py was the first iteration - single layer neural net with no activation
2. nn.py was the next iteration - a more general purpose network supporting multiple layers and activations

More details are in the headers of each file
